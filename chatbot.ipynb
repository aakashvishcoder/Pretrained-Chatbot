{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4baf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:16<00:33, 16.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m     15\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ✅ Add LoRA adapters\u001b[39;00m\n\u001b[32m     24\u001b[39m lora_config = LoraConfig(\n\u001b[32m     25\u001b[39m     r=\u001b[32m16\u001b[39m,\n\u001b[32m     26\u001b[39m     lora_alpha=\u001b[32m32\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:315\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    317\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:4998\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4988\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4989\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4991\u001b[39m     (\n\u001b[32m   4992\u001b[39m         model,\n\u001b[32m   4993\u001b[39m         missing_keys,\n\u001b[32m   4994\u001b[39m         unexpected_keys,\n\u001b[32m   4995\u001b[39m         mismatched_keys,\n\u001b[32m   4996\u001b[39m         offload_index,\n\u001b[32m   4997\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4998\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5004\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5007\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5015\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5016\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:5456\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5453\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5455\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5456\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5457\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5459\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:937\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aakas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:790\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    788\u001b[39m     param = file_pointer.get_slice(serialized_param_name)\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     param = \u001b[43mempty_param\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_device\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# It is actually not empty!\u001b[39;00m\n\u001b[32m    792\u001b[39m to_contiguous, casting_dtype = _infer_parameter_dtype(\n\u001b[32m    793\u001b[39m     model,\n\u001b[32m    794\u001b[39m     param_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m     hf_quantizer,\n\u001b[32m    798\u001b[39m )\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# In this case, the param is already on the correct device!\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# ✅ Load in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ✅ Add LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Qwen uses q_proj/v_proj names\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"Trainable Parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aae4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Question', 'Complex_CoT', 'Response'],\n",
       "        num_rows: 19704\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",'en')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tok(data):\n",
    "    text = [p + \"\\n\" + r for (p, r) in zip(data[\"Question\"], data[\"Response\"])]\n",
    "    model_inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = ds[\"train\"].map(tok, batched=True, remove_columns=[\"Question\", \"Response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de03814",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-lora-output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # simulate batch size 8\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    num_train_epochs=2,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits,axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998c787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_28168\\1124989843.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4926' max='4926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4926/4926 22:35:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>1.212392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.187000</td>\n",
       "      <td>1.177142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4926, training_loss=1.2342733626340494, metrics={'train_runtime': 81330.5847, 'train_samples_per_second': 0.485, 'train_steps_per_second': 0.061, 'total_flos': 8.811840074699244e+17, 'train_loss': 1.2342733626340494, 'epoch': 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14dd2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "metrics_df['eval_loss'].plot(marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training for LLM Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd483551",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"chatbot_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7855be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 21.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\"chatbot_model.pth\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"chatbot_model.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c65b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 42-year-old woman presents complaining of pain in her hands. She reports that the pain is in both hands, and that it is usually worse in the morning. She reports that her hands are also stiff in the morning, but that this gradually improves throughout the morning. She notes, however, that her symptoms seem to be getting worse over the last three months. What is the most likely pathogenesis of her disease process?\n",
      "A. Repetitive microtrauma\n",
      "B. Production of antibodies against smooth muscle\n",
      "C. Production of antibodies against antibodies\n",
      "D. Anti-neutrophil cytoplasmic antibody production\n",
      "E. Autoantibodies against DNA-containing structures\n",
      "The most likely underlying cause for the progressive worsening of hand joint stiffness and morning swelling in your patient would align with Sjögren’s syndrome rather than a classical rheumatic condition like lupus\n"
     ]
    }
   ],
   "source": [
    "prompt = '''A 42-year-old woman presents complaining of pain in her hands. She reports that the pain is in both hands, and that it is usually worse in the morning. She reports that her hands are also stiff in the morning, but that this gradually improves throughout the morning. She notes, however, that her symptoms seem to be getting worse over the last three months. What is the most likely pathogenesis of her disease process?\n",
    "A. Repetitive microtrauma\n",
    "B. Production of antibodies against smooth muscle\n",
    "C. Production of antibodies against antibodies\n",
    "D. Anti-neutrophil cytoplasmic antibody production'''\n",
    "inputs = tokenizer(prompt,return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0],skip_special_tokens=False)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
